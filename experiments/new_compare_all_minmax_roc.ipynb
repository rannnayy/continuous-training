{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update({'font.size': 18})\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_paths = [\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717700456995308991/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717444793172840349/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717705984940495539/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717575413839183617/\",\n",
    "    # \"/mnt/extra/continuous-training/results_fixed/1717467553842856231/\",\n",
    "    \"/mnt/extra/continuous-training/results/1718057739541878219\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717575369578062901/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717579388065196179/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717701022363026467/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717608769994081703/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717579439091810476/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717587649593441377/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717700529336249757/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717635454957034095/\",\n",
    "    # \"/mnt/extra/continuous-training/results_fixed/1717446231823791885/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717448773806529718/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717612290915437273/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717594617566228467/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717478686265173993/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717619549666300929/\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717704913915861438/\",\n",
    "    # \"/mnt/extra/continuous-training/results_fixed/1717456543236185969/\",\n",
    "    \"/mnt/extra/continuous-training/results/1718057735112524876\",\n",
    "    \"/mnt/extra/continuous-training/results_fixed/1717715829899615491/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(columns=['folder_path', 'algo', 'param_path', 'csv_path', 'latencies_path', 'num_retrain', 'training_time_total', 'training_time_single', 'training_cpu_time', 'training_counter', 'inference_time_total', 'inference_time_single', 'inference_cpu_time', 'inference_counter'])\n",
    "for stat in result_paths:\n",
    "    contents = []\n",
    "    with open(os.path.join(stat, 'parameters.txt'), \"r\") as f:\n",
    "        contents = f.readlines()\n",
    "    # Iterate to get variables\n",
    "    folder_path = stat\n",
    "    algo_name = ''\n",
    "    csv_path = ''\n",
    "    latencies_path = os.path.join(folder_path, 'latencies.csv')\n",
    "    param_path = stat\n",
    "    valid_entry = False\n",
    "    retrained = -1\n",
    "    training_time_total = -1\n",
    "    training_time_single = -1\n",
    "    training_cpu_time = -1\n",
    "    training_counter = -1\n",
    "    inference_time_total = -1\n",
    "    inference_time_single = -1\n",
    "    inference_cpu_time = -1\n",
    "    inference_counter = -1\n",
    "    for content in contents:\n",
    "        if not valid_entry and 'Inference' in content:\n",
    "            valid_entry = True\n",
    "        if '-output' in content:\n",
    "            try:\n",
    "                algo_name = content.split(' = ')[1].split('\\n')[0]\n",
    "            except:\n",
    "                algo_name = content.split('=')[1].split('\\n')[0]\n",
    "            csv_path = os.path.join(algo_name+'.csv')\n",
    "        if 'Retrained' in content:\n",
    "            retrained = float(content.split(' = ')[1].split('\\n')[0])\n",
    "        if 'Training time total' in content:\n",
    "            training_time_total = float(content.split(' = ')[1].split(' s')[0])\n",
    "        if 'Training time single' in content:\n",
    "            training_time_single = float(content.split(' = ')[1].split(' s')[0])\n",
    "        if 'Training CPU times usage' in content:\n",
    "            training_cpu_time = float(content.split(' = ')[1].split(' sCPU')[0])\n",
    "        if 'Training counter' in content:\n",
    "            training_counter = float(content.split(' = ')[1].split('\\n')[0])\n",
    "        if 'Inference time total' in content:\n",
    "            inference_time_total = float(content.split(' = ')[1].split(' s')[0])\n",
    "        if 'Inference time single' in content:\n",
    "            inference_time_single = float(content.split(' = ')[1].split(' s')[0])\n",
    "        if 'Inference CPU times usage' in content:\n",
    "            inference_cpu_time = float(content.split(' = ')[1].split(' sCPU')[0])\n",
    "        if 'Inference counter' in content:\n",
    "            inference_counter = float(content.split(' = ')[1].split('\\n')[0])\n",
    "    if valid_entry:\n",
    "        stats_df.loc[len(stats_df)] = [folder_path, algo_name, param_path, csv_path, latencies_path, retrained, training_time_total, training_time_single, training_cpu_time, training_counter, inference_time_total, inference_time_single, inference_cpu_time, inference_counter]\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies_acc_list = {}\n",
    "latencies_rej_list = {}\n",
    "\n",
    "values_df_1 = pd.DataFrame({\n",
    "    'path': pd.Series(dtype='string'),\n",
    "    'algo': pd.Series(dtype='string'),\n",
    "    'length': pd.Series(dtype='int'),\n",
    "    'f1': pd.Series(dtype='object'),\n",
    "    'roc_auc': pd.Series(dtype='object'),\n",
    "    # 'latencies_acc': pd.Series(dtype='object'),\n",
    "    # 'latencies_rej': pd.Series(dtype='object'),\n",
    "    'retrain': pd.Series(dtype='int'),\n",
    "    'training_time_total': pd.Series(dtype='float'),\n",
    "    'training_time_single': pd.Series(dtype='float'),\n",
    "    'training_cpu_time': pd.Series(dtype='float'),\n",
    "    'training_counter': pd.Series(dtype='int'),\n",
    "    'inference_time_total': pd.Series(dtype='float'),\n",
    "    'inference_time_single': pd.Series(dtype='float'),\n",
    "    'inference_cpu_time': pd.Series(dtype='float'),\n",
    "    'inference_counter': pd.Series(dtype='int'),\n",
    "})\n",
    "\n",
    "for i, row in stats_df.iterrows():\n",
    "    print(i)\n",
    "    temp_df = pd.read_csv(os.path.join(row['folder_path'], row['csv_path']), index_col=0)\n",
    "    temp_df = temp_df.drop_duplicates(subset='minute', keep=\"last\")\n",
    "    retrain_list = temp_df['retrain'].tolist()\n",
    "    retrain = [i for i in retrain_list if i==True]\n",
    "    latencies_df = pd.read_csv(row['latencies_path'], header=None, names=['latency', 'reject'], dtype={'latency': int, 'reject': str})\n",
    "    latencies_acc = latencies_df[latencies_df['reject'].isin([\"0\", \"True\"])]['latency'].tolist()\n",
    "    latencies_acc_list[row['algo']] = latencies_acc\n",
    "    latencies_rej = latencies_df[latencies_df['reject'].isin([\"1\", \"False\"])]['latency'].tolist()\n",
    "    latencies_rej_list[row['algo']] = latencies_rej\n",
    "    # if temp_df['minute'].max() == 480:\n",
    "    # if i != 3:\n",
    "    #     values_df_1.loc[len(values_df_1)] = [row['folder_path'], row['algo'], temp_df['minute'].max(), temp_df['f1_score'].tolist()[:-5], temp_df['roc_auc'].tolist()[:-5], row['num_retrain'], row['training_time_total'], row['training_time_single'], row['training_cpu_time'], row['training_counter'], row['inference_time_total'], row['inference_time_single'], row['inference_cpu_time'], row['inference_counter']]\n",
    "    # else:\n",
    "    values_df_1.loc[len(values_df_1)] = [row['folder_path'], row['algo'], temp_df['minute'].max(), temp_df['f1_score'].tolist(), temp_df['roc_auc'].tolist(), row['num_retrain'], row['training_time_total'], row['training_time_single'], row['training_cpu_time'], row['training_counter'], row['inference_time_total'], row['inference_time_single'], row['inference_cpu_time'], row['inference_counter']]\n",
    "\n",
    "# print(values_df_1.iloc[13]['path'])\n",
    "# values_df_1.drop([13], inplace=True)\n",
    "values_df_1.reset_index(inplace=True, drop=True)\n",
    "values_df_1.to_csv('values_all_minmax_latencies.csv')\n",
    "values_df_1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
